# условная энтропия
Сколько случайности в Х, если нам дана Y

H(X \\mid Y) = 

\- \\sum\_{i, j} \\mathbb{P} (X = x\_i, Y = y\_j) \\log \\mathbb{P} (X = x\_i \\mid Y = y\_j)

связано с [информационная энтропия случайной величины](%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F%20%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F%20%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D0%BE%D0%B9%20%D0%B2%D0%B5%D0%BB%D0%B8%D1%87%D0%B8%D0%BD%D1%8B)