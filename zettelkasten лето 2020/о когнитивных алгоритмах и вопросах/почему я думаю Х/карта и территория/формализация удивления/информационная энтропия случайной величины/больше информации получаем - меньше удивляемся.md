# больше информации получаем - меньше удивляемся
Наибольшая энтропия соответствует равномерному распределению, то есть случаю, когда мы совсем не обладаем априорными знаниями о данных. Большая энтропия соответствует большему нашему незнанию. Чем больше мы получаем информации от среды и пользуемся формулой Байеса для обновления наших вероятностей, тем меньше становится наше ожидаемое удивление.

связано с [информационная энтропия случайной величины](%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F%20%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F%20%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D0%BE%D0%B9%20%D0%B2%D0%B5%D0%BB%D0%B8%D1%87%D0%B8%D0%BD%D1%8B)

связано с [Теорема Байеса](%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0%20%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%B0)