[link](https://youtu.be/jiZxEJcFExc)
ЭА в значительной степени уже фокусируются на рисках полного уничтожения человечества. В данной лекции будут рассмотрены возможности, которые, возможно, даже хуже полного уничтожения.
Риски крупномасштабного тяжелого страдания в будущем.
Рассмотрим серию *Black Mirror*. В ней сознания разумных существ возможно загружать и запускать на небольших электронных девайсах. При этом их можно пытать.
Любой сценарий, который мы представим, имеет маленькую вероятность случиться. Однако сценариев, вероятно, много.

Экзистенциальные риски - неблагоприятный исход, при котором цивилизация полностью уничтожается, или наносится перманентный вред ее потенциалу.
*"One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential"*
Nick Bostrom
dimentions: Severity vs scope

Жизнь цыпленка настолько плоха, что лучше бы он не рождался. МОжет быть, подобные вещи способны случиться для человечества.
the worst possible severity - hellish involuntary suffering
the greatest possible scope - cosmic

Возможный аргумент, который автор *не* делает
*Посылка 1 - лучшая вещь - уменьшать наихудшие риски
Посылка 2- s-риски - наихудшие риски
Заключение: лучшая вещь уменьшать s-риски*

Есть больше возможных критериев по выбору наиболее этичных действий, помимо ужасности и масштаба.
- Насколько вероятно появление
- Насколько tractable
- Насколько neglected

### Вероятность
Такие риски не менее вероятны, чем риски полного уничтожения от сильного ИИ.
Контраргументы
- это слишком абсурдно
	- интуитивные оценки вероятностей, согласно исследованиям из области эвристик и искажений ([[рациональность]]), зачастую происходят засчет эвристики доступности - насколько легко нам вспомнить тот или иной случай
	- соответственно, если ни разу что-то не происходило, мы принижаем вероятность

Два возможных технологичесикх изобретения, ведущих к s-рискам.

### Artificial sentience
Может быть, возможность испыывать субъетивный опыт и страдать не является характерной особенностью биологического субстрата и может быть реализована на компьютере. 
Whole Brain Emulation может к этому привести.

### Superintelligent AI
Nick Bostrom, Superintelligence
Может создать много дополнительных технологических возможностей. Вз-ие между SAI и AS может привести к s-risks.

Может быть, это все равно маловероятно. Нужен мощный агент, который будет реализовывать свою злую волю?

Еще это может произойти по ошибке. К примеру, создание существ, которые могут страдать, но не могут коммуницировать.

*Paperclip maximizer.* Что если такой агент будет запускать осознающие себя программы, которые будут страдать?

**Часть конфликта**

Есть немного дополнительных предположений, которые нужно сделать, чтобы перейти от признания концептуальной возможности рисков от ИИ к возможности s-рисков.

## Tractable?
Насколько легко уменьшать такие риски? Это хотя бы немного возможно уже сейчас. Работа в Techincal AI Safety и AI Policy. Работа, при которой понижается вероятность многополярных сценариев и увеличивается вероятность однополярного сценария.

Усиление международной кооперации. Расширение морального круга и включение в него искусственных существ с сознанием.

Можем ли мы получать поддержку в виде грантов? Это кажется слишком странным для большинства людей. История поля AI Safety говорит, что возможно преодолеть это.

## Neglectedness
S-рискам меньше уделяется внимания, чем х-рискам. Некоторые думают, что х-риски - это то же самое, что экзистенциальные риски.

Видение сообщества, которое будет влиять на долгосрочное будущее
Это долгое путешествие. Некорректно считать, что перед нами утопия или глобальная катастрофа. Нас ждет континуум вариантов. Как формировать будущее? Кого-то мотивирует желание избежать катастрофы, кого-то желание попасть в великое благополучие.

# Вопросы
Что есть помимо WBE и AS? У кремниевого субстрата есть много преимуществ в сравнении с биосубстратом.

Почему мы можем придавать моральный вес небиологическому существу? Мысленный эксперимент. Последовательно заменяем человеку нейроны в мозге чипами. Он не станет ни в какой момент менее ценным.* Philisophy of Mind* 

Почему для супер-ИИ было бы нужно эмулировать AS с страданием? Кажется мало вероятным, что современные алгоритмы машинного обучения будут придавать значение страданию.

Возможно, агенты могут предпочесть существование с сильным страданием смерти? Да, возможно.










[[оценки вероятностей x-рисков]]




