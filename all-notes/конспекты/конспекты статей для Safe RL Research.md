# SAFETY VERIFICATION OF MODELBASED REINFORCEMENTLEARNING CONTROLLERS

В статье разрабатывается метод чтобы DRL модели не попадали в нежелательные зоны при своей работе.
Model-based RL не имеет априорных знаний о среде и строит модель при обучении. Если в реальность заложена стохастичность, то и модель может быть неточной и непредсказуемой. Нам нужно суметь доказать, что контролер безопасен, до того как пускать его в среду. 
Алгоритм forward reachable tube и backward reachable tube. Благодаря ним мы выявляем состояния среды, в которых агенту находиться безопансо.

Для численных экспериментов в качестве основы используется https://github.com/HJReachability/helperOC

# Online Safety Assurance for Deep Reinforcement Learning
Online Safety Assurance Problem (OSAP)
Представлены 3 способа подсчитывать неуверенность алгоритма в предсказаниях.
Известный недостаток нейросетей - невозможность обобщать на распределения данных, которые отличаются от распределения тестовых данных. Авторы вкладывают в агента алгоритм для определения, насколько сильно он не уверен. Как лучше всего понимать неуверенность?
- неуверенность в стстоянии среды
- несоответствующие друг другу выборы действий
- некорректное восприятие того как выборы влияют на поведение
Мы находимся в MDP. 
Разбирается пример с передачей видео по двум известным алгоритмам online bitrate adaptation: Pensieve, Buffer-based

Как именно мы можем измерять неуверенности. 
$U_S$ novelty detection
$U_{\pi}$ ансамбли агентов
$U_V$ ансамбли функций

# CONSERVATIVE SAFETY CRITICS FOR EXPLORATION


# Neurosymbolic Reinforcement Learning with Formally Verified Exploration
непрерывное пространство действий и состояний 
используют два класса полиси
1. общий нейросимволический класс с приближенными градиентами
2. более ограниченный класс, допускающий формальную верификацию

алгоритм - mirror descent 
на каждой итерации что-то там происходит

есть методы с шилдами, которые пока работают только на игрушечных примерах

в данной рапболте тоже используется мониторинг и шилдинг, но они не постоянные, а обновлшяются в процессе обчения

обновление идет на каждом шаге; верификация нейросети весьма вычислительно сложна, поэтому используется символьная форма ее представления

основа - Propel, софт в котором есть символьное представление, но нет забот о безопасности

как работает алгоритм
1. подъем шилда 
2. обновление нейросимвольной полиси через приблизительные градиенты
3. проецирование обнолвенной полиси обратно

[link](https://github.com/gavlegoat/safe-learning)

Preliminaries

Формальная верификация. Определяем множество 