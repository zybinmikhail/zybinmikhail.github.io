# GPT-3
GPT-3 не имеет существенных алгоритмических улучшений в сравнении с GPT-2. Дело в том, что у третьей модели 175 миллиардов параметров, а у второй 1.5.

Примеры третьей https://read-the-samples.netlify.app/

Примеры второй https://openai.com/blog/better-language-models/

Вторая не умеет делать арифметику, а третья умеет.

Складывать и вычитать двухзначные числа с аккуратностью больше 90%б трехзначные больше 80%.

Чтобы арифметика проходила лучше, модели следует “напомнить” про сложение, предоставив несколько примеров.

Ошибки в вычислениях похожи на человеческие - нам тоже сложнее в уме складывать большие числа, чем маленькие.

Вопрос - как связано увеличение параметров модели с ее качеством? Можно ли, увеличивая количество параметров, добиться сколь угодно большого качества?

Пока кажется, что прирост логарифмический по количеству параметров.

Люди могут отличить написанный третьей моделью текст от написанного человеком в 52% случаев.

Эти модели решают задачу предсказания следующего слова в тексте.

Возможно, масштабирование моделей и соответственный рост качества - общее для машинного обучения явление, которое еще много где и долго будет наблюбдаться.

В 2010 году почти никто не интересовался глубоким обучением. За десятилетие произошло нечто, сравнимое с Кембрийским взрывом по скорости прогресса и обилию новых подходов.

Эксперты в области ИИ в течение прошедшего десятилетия предсказывали, что существующий хайп скоро сменится новой AI winter. Этого не происходит, эксперты ошибаются.

https://www.gwern.net/newsletter/2020/05

https://webcache.googleusercontent.com/search?q=cache:cHkqQM2aZSgJ:https://slatestarcodex.com/2020/06/10/the-obligatory-gpt-3-post/

https://nostalgebraist.tumblr.com/post/619672884731904000/gpt-3-and-scaling-trends

https://blog.piekniewski.info/2018/08/28/fun-numbers-about-the-brain/