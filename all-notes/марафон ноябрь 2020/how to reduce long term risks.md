*9 ноября 2020*
моя цель - принесение блага как можно большему числу разумных существ. При это для меня ценность разумного существа не зависит от того, где и когда оно находится.Человечество находится в начала своего пути, и его ждут еще миллионы лет жизни и развития, поэтому в будущем, если все будет складываться благополучно, будет жить огромное количество людей. Следовательно, я хочу фокусироваться на том, чтобы такой исход - миллионы лет благополучия - действительно имел место. В данной заметке я разберу известные мне идеи о том, как уменьшать долгосрочные риски, связанные с великой судьбой человечества.

Экзистенциальный риск (х-риск) - это риск полного невосстановимого уничтожения цивилизации. Какие риски я могу выделить
- глобальная пандемия
- падение большого небесного тела
- извержение супервулкана
- недружественный сильный ИИ
- вышедшие из-под контроля наноассемблеры
- масштабная война с использованием ядерного оружия
- ...

s-риски - это риски, связанные с вхождением цивилизации в состояние, из которого тяжело выйти, при котором много разумных существ испытывают сильные страдания. Главный пример - люди становятся бесправными рабами у диктатора (человека или ИИ), который поддерживает свою власть с помощью мощных технологий. Возможно, стоит разделить эти две возможности - человек или группа людей используют технологии, из-за которых большинство страдает, или технологии сами по себе приводят к тому, что страдают все. В первом случае первая идея, приходящая в голову - усиление этичности людей, во втором случае - внедрение законопроектов, не позволяющих появляться опасным технологиям.

Смежные важные вопросы - что такое сознание, что такое страдание? Возможно ли усиление этичности в глобальном масштабе? Возможны ли s-риски, связанные с страданиями симулируемых на компьютере людей? Как понять, есть ли страдание у ИИ?