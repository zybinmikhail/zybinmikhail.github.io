Лекция была о гауссовой элиминации, о LU-разложении, о QR-разложении.
$A = LU$, где L - нижнетреугольная матрица, U - верхнетреугольная. Разложение может быть не единственно, поэтому требуем чтобы все диагональные элементы L были единичными.
Существование разложения эквивалентно тому, что все миноры на главной диагонали ненулевые.
Положительно определенная матрица в комплексном случае обязана быть эрмитовой ($A = A^*$)
Положительно определенная матрица в действительном случае не обязана быть симметричной.
Положительно определенная матрица имеет все ненулевые миноры на главной диагонали, и для нее разложение LU превращается в разложение Холеского: $R R^*$.

Можно условно выделить
* небольшие матрицы размером $n < 10^6$
* средние матрицы
	* в них можно искать структуру
* большие матрицы
	* спарс

Спарс-матрица - матрица, в которой достаточно много нулевых элементов, чтобы их наличие можно было использовть для ускорения алгоритмов работы с этой матрицей.

Вычисление детерминанта - плохая идея.

Когда тренируем модель, пишем model.train()
Когда оцениваем, пишем model.eval()

Если нам нужны градиенты тензора, то при инициализации пишем requires_grad=True
with torch.no_grad(): делает так, что градиенты не считаются
когда обучаем модель, то делаем вот что
- обнуляем градиненты
- считаем значение функции потерь
- loss.backwards()
- optimizer.step()
