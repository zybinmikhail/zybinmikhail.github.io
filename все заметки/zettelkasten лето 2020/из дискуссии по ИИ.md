# из дискуссии по ИИ
Оптимизационный процесс в общем случае - это процесс попадания в маленькую цель в большом пространстве. Под уровнем интеллекта системы можно как раз понимать, в насколько маленькие цели она может попадать. Примеры, которые у нас сейчас есть - это люди и эволюция.

Может быть, люди скоро создадут сущность (неважно, как она будет выглядеть), которая будет очень-очень хорошо уметь влиять на мир так, что мир будет попадать в маленький участок возможных вариантов будущего.

Для нас, людей, важно, чтобы сильный ИИ переводил мир в такие варианты будущего, которые для нас благоприятны. Априори сила ИИ, его оптимизационная мощь, никак не связана с этичностью этого ИИ с точки зрения человеческих ценностей.

Защититься можно, если как-то суметь доказать, что в эту оптимизацию заложены человеческие ценности.

Я скорее думаю, что чем абстрактнее знание, тем к большему количеству ситуаций оно приложимо. По крайней мере, в данной ситуации для нас неважна конкретная архитектура ИИ.

В то, как ИИ выбирает, какие варианты мира ему предпочтительнее. Это может быть функция полезности.

Да, то, чего мы боимся, еще не создано. Может быть, оно и не будет создано.

один из важных вопросов в ai safety - понять, какие классы алгоритмов вообще могут следовать заданной функции полезности, а не съезжать с рельс в произвольном направлении

другие вопросы:

\- как эту функцию вообще кодировать (как формализовать, чего хотят люди)

\- policy: как организовать человеческие институты, чтобы никто никому с помощью ИИ не навредил

\- философия: что вообще мы имеем в виду под функцией полезности и одно ли и то же имеем в виду, и если она у разных людей разная, как о ней договориться (это уже не ai safety, правда, ближе к утилитаризму и метаэтике)

\- проблема контроля: как нам запустить на пробу ИИ, который может оказаться не очень хорошим, чтобы мы потом могли его безопасно выключить и он на это согласился, а не стал нам мешать

Отдельные люди плохо умеют координироваться между собой, в частности потому что bandwidth между людьми очень узкий, и поэтому стоит ожидать, что группа ИИ будет легко переигрывать группы людей.