Уже было 4 лекции. 
# Лекция 1
На первой лекции разбирались такие понятия, как флопс, fixed point arithmetics, floating point arithmetics.
floating point operations per second - количество операций над числами с плавающей запятой в секунду

число с плавающей запятой имеет вид $(-1)^{\text{sign}}\cdot a \cdot b^c$
фиксированная точка - хорошая абсолютная точность, но все очень плохо если совершать операции над числами разного порядка
плавающая точка - хорошая относительная точность

есть разные типы данных 
* half precision
	* 16 бит
* single precision
	* 32 бита
* double precision
	* 64 бита
* tensorfloat

Возможные типы данных защиты в hardware. Для некоторых приложений глубокого обучения подходят типы данных half precision или другие. В градиентном спуске и так числа приближенные, поэтому слишком точные значения хранить не имеет смысла. Засчет меньшей точности модели меньше весят и быстрее учатся.

Форвардная ошибка, бэквордная ошибка ???

Главные операции в линейной алгебре - умножение вектора на матрицу и умножение матрицы на матрицу.
У векторов бывают нормы. $(\sum_1^n ||x_i||^p)^{1/p}$

Из этих норм 2-норма порождена скалярным произведением
Вот операторная норма:
$$||A||_{*, **} = \sup_{x \neq 0} \frac{||Ax||_{*}}{||x||_{**}}$$
Не все нормы порождаются каким-либо оператором.
У матриц тоже бывают нормы. 
Фробениусова норма - корень из суммы квадратов всех элементов.
*Субмультипликативность нормы* - $\forall A, B ||AB|| \leq ||A|| ||B||$
Несубмультипликативная норма - _норма Чебышева_ - максимальный модуль элемента матрицы.
Матричная p-норма - когда $||x||_* = ||x||_{**} = ||x||_p$
p=1 - максимальная сумма по строкам
$p=\infty$ - максимальня сумма по столбцам
p=2 - спектральная норма, она же максимальное сингулярное значение, она же корень из максимального собственного значения матрицы $A^*A$.
Оператор $U$ называется *унитарным*, если 
* $\forall x, y \langle U x, U y \rangle = \langle x, y \rangle$
* $\forall x ||Ux||_2 = ||x||_2$
* $U^*U = I$
Спектральная норма и фробениусова норма унитарно инвариантны.
Если некоторый алгоритм представляет собой унитарное преобразование, то это хорошо, потому что в таком случае численная погрешность не накапливается.
Есть разложения матриц. В первой лекции, где они были упомянуты, их было упомянуто два.
## QR-разложение
_Матрица Хаусхолдера_ $H_v = I - 2v v^*$
Может занулить все элементы вектора кроме первого.
Одно из них - кажется, что любая матрица может быть разложена в произведение унитарной и верхнетреугольной.
В одном фигурирует отражение вектора относительно плоскости, нормалью к которой является заданный вектор.
Некоторым образом засчет возможности зануления элементов матрицы она приводится к верхнетреугольному виду унитарными преобразованиями.
## Разложение Гивенса
повороты
больше подходят для спарс-матриц, потому что мы зануляем разные элементы независимо, и можно распараллеливать

**Алгоритм Штрассена ** умножения матриц имеет асимптотическую сложность $\mathcal{O}(n^{\log_2 7})$ и достигает ее засчет парадигмы *divide-and-conquer*.
Скорость алгоритма во многом зависит от того, насколько часто ему требуется переносить данные из памяти в процессор. Есть процессор, несколько уровней кеша (L1, L2, L3), есть твердотельная память. Чем быстрее память, тем меньше в нее можно положить.
Матричное умножение требует мало переносов.

В курсе для решения практических задач мы будем пользоваться библиотекой *jax*. Она позволяет значительно ускорять и оптимизировать питоновский код. В том числе в ней содержится весь функционал библиотеки numpy (jax.numpy).
Также jax позволяет осуществлять автоматический подсчет градиентов

**Ранг матрицы** можно определять следующими способами
* столбцовый ранг - количество линейно независимых столбцов
* строчный ранг - количеств линейно независимых строк
* наименьшее количество матриц с разделяющимися индексами, дающих в сумме исходную матрицу

Данные определения эквивалентны для двумерных матриц и неэквивалентны для тензоров более высоких порядков.

### Низкоранговая аппроксимация
Если нам точно известно, что матрица имеет ранг r, то имеет место следующее _скелетное_ разложение:
$$A = C \hat{A}^{-1} R$$,
$C$ - некоторые r линейно независимых столбцов, $R$ - некоторые r линейно независимых строк, $\hat{A}$ - матрица из элементов, находящихся на их пересечении.
Как вариант приближенного разложения можно взять только часть столбцов (строк) из матриц C и R.
В данном случае порог, по которому следует обрезать, можно определить, глядя на уменьшение сингулярных значений. ???