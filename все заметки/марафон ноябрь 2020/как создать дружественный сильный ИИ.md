Создание сильного ИИ, если оно произойдет, станет поворотным моментом в истории человеческой цивилизации. Дальнейшие пути развития - либо утопия и благоденствие, либо глобальная катастрофа. Поэтом проблема того, чтобы системы, которые будут обладать большим интеллектом, чем люди, были безопасными для нас, стоит чрезвычайно остро. В данной заметке я проведу размышление о возможных способах создания таких систем, цели которых соответствовали бы целям людей, и которые при этом превосходили людей по способностям решать задачи.

Сразу же отмечу, что алгоритмы машинного обучения, в частности глубокого обучения, получающие широкое общественное внимание в последние годы - это лишь крохотная часть обширного математического аппарата, которым обдалает человечество. А именно, глубокое обучение - часть вычислительной параметрической статистики, которая включается в теорию вероятностей, которая включается в теорию меры, которая включается в математический анализ. Если ставить общую задачу создать искусственный ум, то совсем неочевидно, что именно указанная часть математики в этом поможет больше всего. О критике глубокого обучения подробнее в статье [Gary Marcus, *Deep Learning: A Critical Appraisal*](https://arxiv.org/abs/1801.00631).

Существует два подхода к пониманию мышления - символизм (symbolism) и коннекционизм (connectionism). Символистский взгляд состоит в том, что мышление можно свести к определенному манипулированию символами - к примеру, совершению логических операций. С этим связана аксиоматизация и формализация математики - в ней зачастую доказательства подчинены строгим законам, и достаточно бывает знать только аксиомы и правила вывода, чтобы проверить верность доказательства, а смысл знать необязательно.

В 1980-е годы набрали популярность так называемые *экспертные системы* - алгоритмы, которые, основываясь на сложных наборах символьных правил, помогали людям принимать решения. Подобные системы были очень негибкими, и в конечном счете несоответствие ожиданий и реальности привели к спаду популярности и зиме ИИ.

Существует множество способов понимать, что такое интеллект. В данной заметке мы будем понимать это, неформально выражаясь, как способность системы попадать в небольшую цель в большом пространстве. (о других возможных способах см. [Shane Legg, Marcus Hutter, *A Collection of Definitions of Intelligence*](https://arxiv.org/abs/0706.3639)).

Интеллектуальный агент в общем понимании должен иметь сенсоры (устройства, с помощью которых он получает информацию из среды) и акторы (устройства, с помощью которых он воздействует на среду) и алгоритм принятия решений - что делать акторами на основании информации с сенсоров. У агента может быть модель среды.

У людей нет единой модели реальности, которой бы они всегда пользовались независимо от обстоятельств - вместо этого мы создаем множество моделей отдельных участков, которые при этом могут иметь [иерархическую структуру](https://www.lesswrong.com/s/p3TndjYbdYaiWwm9x). В разных ситуациях требуется разная детализированность. На тело человека можно посмотреть как на скопление элементарных частиц, скопление атомов, скопление молекул, или клеток, или тканей, или органов, или систем органов, или как на ограниченно рационального агента, причем во всех случаях человек один и тот же. Вопрос - как можно добиться того, чтобы искусственно созданная система могла бы так же создавать и модифицировать множество разных моделей участков реальности?

Возможная гипотеза, объединяющая символизм и коннекционизм - что верхнеуровнево мышление можно считать символьным, а на низком уровне оно коннекционистское. Но как из поведения отдельных нейронов складывается мышление? Должен ли ИИ тоже состоять из сети связанных друг с другом не значащих ничего самих по себе деталей?

Возможный вариант - создать систему, не обладающую высоким интеллектом изначально, но способную к обучению или самообучению. 

Распространенный сейчас вариант слабого ИИ - компьютерная программа с набором данных, которая запускается на компьютере фон Неймана. Возможно, нам нужно что-то совсем другое. Мне кажется важным правило "fire-wire". А именно, если человек несколько раз каким-то образом воспринимает некоторую сущность, то у него возбуждаются примерно одинаковые нейроны. Засчет совместного возбуждения между ними возникают связи, и таким образом в мозгу формируется "концепция" воспринимавшейся сущности. Одинаковых предметов не бывает, и похожесть представляет собой на уровне мозга активацию примерно одинаковых нейронов. Можно ли создать искусственную систему, которая обладала бы похожей "гибкостью", в которой могли бы "отражаться" аналогичным образом вещи?

Помимо моделирования, есть цели. Откуда берутся цели у людей, как вложить цели в ИИ? Стоит отметить, что у людей, когда они умнеют (в том числе взрослеют) меняются цели, поэтому неудивительно было бы пронаблюдать это в случае усиливающегося ИИ. Как добиться того чтобы ИИ сохранил изначально вложенные в него цели?

Важные желаемые свойства сильного ИИ - робастность и интерпретируемость. Робастность означает, что поведение системы меняется мало, если входные данные меняются мало. Интерпретируемость означает, что причины, по которым система приняла то или иное решение, могут быть поняты людьми. Глубокие искусственные нейронные сети неробастны и неинтерпретируемы.