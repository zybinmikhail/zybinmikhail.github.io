Обща яканва - мтоды безопасного RL. Мне следует делать ресерч в сфере формальной верификации. Суть в том, чтобы мы могли на языке логики написать ограничения, которым должна соответствовать траектория агента в пространстве состояний, а также доказать, что эти ограничения выполняются.

На данный момент я потратил некоторое время на изучение так называемой динамической дифференциальной логики. У нее, как и у всякой логики, есть синтаксис (правила конструирования корректных формул) и семантика (правила присвоения смысла формулам). я не хочу вдаваться в подробности здесь, лишь упомяну, что мы имеем дело не только с формулами, но и с термами и программами.

В данной работе акцент делается на *гибридные системы* - динамические системы, в которых есть и непрерывная, и дискретная компонента. 

$P \to [\alpha^*] Q$

смысл вышезаписанной формулы в следующем: если выполнено начальное условие $P$, то после любого количества запусков программы  $\alpha$ будет выполнено условие $Q$.

Оставлю без доказательства утверждение: для алгоритмов, относящихся к RL без нейросетей, как правило достаточно легко показать их безопасность, однако в глубоком RL это становится чрезвычайно сложной задачей.

Автор динамической дифференциальной логики, Андре Плацер создал KeYmaeraX - помощника в доказательстве теорем. В нем содержится большое количество лемм и правил вывода. Мне пока не удалось понять, как использовать полный функционал этой  программы. 

Есть Contoller Monitor и еще какой-то монитор, который я не помню как называется и что делает. 

$\mathbb{J}$ - это Bellman error

делаем щит
он с химерой не работает
прочувствовать самому химеру, четко разобрать один пример

числа 15 созвон

следующие действия
- разобрать действие химеры на конкретном примере формулы
- понять что такое щит и почему он с химерой не работает

Читаем пейпер.
Safe reinforcement learning via shielding
Авторы предлагают использование щита, причем в двух разных конфигурациях. До обучаегося агента и после.

В статье используется темпоральная логика.

Щит отслеживает действия агента и блокирует их титтк считает их небезопасными.

В случае *до-агента* щит формирует множество всех возможных безопасных действий.

Важные свойства - корректность и минимальное влияние. Щиты из статьи, конечно же, обладают этим свойством. Для доказательства используется сведение к играм.

Есть гитхаб.

Safe Reinforcement Learning via Formal Methods:
Здесь написано про justified Specu-lative Control(JSC)